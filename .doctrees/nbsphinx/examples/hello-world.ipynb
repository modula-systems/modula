{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286c88-ce33-4be7-8aec-3c3fe5176c40",
   "metadata": {},
   "source": [
    "# Hello, World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7a804b-06ec-4773-864c-db8a3b01c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "input_dim = 28 * 28\n",
    "output_dim = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Generate random training data\n",
    "key = jax.random.PRNGKey(0)\n",
    "inputs = jax.random.normal(key, (input_dim, batch_size))\n",
    "targets = jax.random.normal(key, (output_dim, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a14a1b-1428-4432-8e89-6b7cfed3d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 3 atoms and 2 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1\n",
      "...contributes proportion 3 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "from modula.atom import Linear\n",
    "from modula.bond import ReLU\n",
    "\n",
    "width = 256\n",
    "\n",
    "mlp = Linear(output_dim, width)\n",
    "mlp @= ReLU() @ Linear(width, width) \n",
    "mlp @= ReLU() @ Linear(width, input_dim)\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "mlp.jit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "080bbf4f-0b73-4d6a-a3d5-f64a2875da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss 0.9790326952934265\n",
      "Step 100, Loss 0.0018738203216344118\n",
      "Step 200, Loss 0.0014391584554687142\n",
      "Step 300, Loss 0.0010814154520630836\n",
      "Step 400, Loss 0.0008106177556328475\n",
      "Step 500, Loss 0.0005738214822486043\n",
      "Step 600, Loss 0.0003808117180597037\n",
      "Step 700, Loss 0.00022766715846955776\n",
      "Step 800, Loss 0.00011454012565081939\n",
      "Step 900, Loss 3.979807297582738e-05\n"
     ]
    }
   ],
   "source": [
    "from modula.error import SquareError\n",
    "\n",
    "steps = 1000\n",
    "learning_rate = 0.1\n",
    "error = SquareError()\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "w = mlp.initialize(key)\n",
    "w = mlp.project(w)\n",
    "\n",
    "for step in range(steps):\n",
    "    # compute outputs and activations\n",
    "    outputs, activations = mlp(inputs, w)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = error(outputs, targets)\n",
    "    \n",
    "    # compute error gradient\n",
    "    error_grad = error.grad(outputs, targets)\n",
    "    \n",
    "    # compute gradient of weights\n",
    "    grad_w, _ = mlp.backward(w, activations, error_grad)\n",
    "    \n",
    "    # dualize gradient\n",
    "    d_w = mlp.dualize(grad_w)\n",
    "\n",
    "    # compute scheduled learning rate\n",
    "    lr = learning_rate * (1 - step / steps)\n",
    "    \n",
    "    # update weights\n",
    "    w = [weight - lr * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
