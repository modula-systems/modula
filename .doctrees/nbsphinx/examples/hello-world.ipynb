{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286c88-ce33-4be7-8aec-3c3fe5176c40",
   "metadata": {},
   "source": [
    "# Hello, World!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847730fa-390b-4b0a-8600-55fb76f9cc38",
   "metadata": {},
   "source": [
    "On this page, we will build a simple training loop to fit an MLP to some randomly generated data. We start by sampling some data. Modula uses JAX to handle array computations, so we use JAX to sample the data. JAX requires us to explicitly pass in the state of the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7a804b-06ec-4773-864c-db8a3b01c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "batch_size = 128\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "inputs = jax.random.normal(key, (input_dim, batch_size))\n",
    "targets = jax.random.normal(key, (output_dim, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809ea7f-cd49-4b2f-98a9-0bcd420fbcac",
   "metadata": {},
   "source": [
    "Next, we will build our neural network. We import the basic Linear and ReLU modules. And we compose them by using the `@` operator. Calling `mlp.jit()` tries to make all the internal module methods more efficient using [just-in-time compilation](https://jax.readthedocs.io/en/latest/jit-compilation.html) from JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a14a1b-1428-4432-8e89-6b7cfed3d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 3 atoms and 2 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1\n",
      "...contributes proportion 3 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "from modula.atom import Linear\n",
    "from modula.bond import ReLU\n",
    "\n",
    "width = 256\n",
    "\n",
    "mlp = Linear(output_dim, width)\n",
    "mlp @= ReLU() \n",
    "mlp @= Linear(width, width) \n",
    "mlp @= ReLU() \n",
    "mlp @= Linear(width, input_dim)\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "mlp.jit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5fc30",
   "metadata": {},
   "source": [
    "Next, we set up a loss function and create a jitted function for both evaluating the loss and also returning its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b719f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(w, inputs, targets):\n",
    "    outputs = mlp(inputs, w)\n",
    "    loss = ((outputs-targets) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "mse_and_grad = jax.jit(jax.value_and_grad(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b8252-b3f0-4d16-9b48-9d8d582c1abe",
   "metadata": {},
   "source": [
    "Finally we are ready to train our model. We will apply the method `mlp.dualize` to the gradient of the loss to solve for the vector of unit modular norm that maximizes the linearized improvement in loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080bbf4f-0b73-4d6a-a3d5-f64a2875da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0 \t Loss 0.976154\n",
      "Step 100 \t Loss 0.001773\n",
      "Step 200 \t Loss 0.001371\n",
      "Step 300 \t Loss 0.001002\n",
      "Step 400 \t Loss 0.000696\n",
      "Step 500 \t Loss 0.000453\n",
      "Step 600 \t Loss 0.000282\n",
      "Step 700 \t Loss 0.000152\n",
      "Step 800 \t Loss 0.000061\n",
      "Step 900 \t Loss 0.000011\n"
     ]
    }
   ],
   "source": [
    "steps = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "w = mlp.initialize(key)\n",
    "\n",
    "for step in range(steps):\n",
    "\n",
    "    # compute loss and gradient of weights\n",
    "    loss, grad_w = mse_and_grad(w, inputs, targets)\n",
    "    \n",
    "    # dualize gradient\n",
    "    d_w = mlp.dualize(grad_w)\n",
    "\n",
    "    # compute scheduled learning rate\n",
    "    lr = learning_rate * (1 - step / steps)\n",
    "    \n",
    "    # update weights\n",
    "    w = [weight - lr * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:3d} \\t Loss {loss:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
