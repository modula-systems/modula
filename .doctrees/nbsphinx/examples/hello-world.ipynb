{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286c88-ce33-4be7-8aec-3c3fe5176c40",
   "metadata": {},
   "source": [
    "# Hello, World!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847730fa-390b-4b0a-8600-55fb76f9cc38",
   "metadata": {},
   "source": [
    "On this page, we will build a simple training loop to fit an MLP to some randomly generated data. We start by sampling some data. Modula uses JAX to handle array computations, so we use JAX to sample the data. JAX requires us to explicitly pass in the state of the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7a804b-06ec-4773-864c-db8a3b01c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "batch_size = 128\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "inputs = jax.random.normal(key, (input_dim, batch_size))\n",
    "targets = jax.random.normal(key, (output_dim, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809ea7f-cd49-4b2f-98a9-0bcd420fbcac",
   "metadata": {},
   "source": [
    "Next, we will build our neural network. We import the basic Linear and ReLU modules. And we compose them by using the `@` operator. Calling `mlp.jit()` tries to make all the internal module methods more efficient using [just-in-time compilation](https://jax.readthedocs.io/en/latest/jit-compilation.html) from JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a14a1b-1428-4432-8e89-6b7cfed3d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 3 atoms and 2 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1\n",
      "...contributes proportion 3 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "from modula.atom import Linear\n",
    "from modula.bond import ReLU\n",
    "\n",
    "width = 256\n",
    "\n",
    "mlp = Linear(output_dim, width)\n",
    "mlp @= ReLU() \n",
    "mlp @= Linear(width, width) \n",
    "mlp @= ReLU() \n",
    "mlp @= Linear(width, input_dim)\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "mlp.jit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af8a3d-77dc-4007-a6de-03ab617bf3fa",
   "metadata": {},
   "source": [
    "Next, we choose our error measure. Error measures allow us to both compute the loss of the model, and also to compute the derivative of the loss with respect to model outputs. For simplicity we will just use squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ea38a1-2684-437f-88fc-cb1f2a44133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.error import SquareError\n",
    "\n",
    "error = SquareError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b8252-b3f0-4d16-9b48-9d8d582c1abe",
   "metadata": {},
   "source": [
    "Finally we are ready to train our model. The method `mlp.backward` takes as input the weights, activations and the gradient of the error. It returns the gradient of the loss with respect to both the model weights and the inputs. The method `mlp.dualize` takes in the gradient of the weights and solves for the vector of unit modular norm that maximizes the linearized improvement in loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080bbf4f-0b73-4d6a-a3d5-f64a2875da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0 \t Loss 0.976274\n",
      "Step 100 \t Loss 0.001989\n",
      "Step 200 \t Loss 0.001537\n",
      "Step 300 \t Loss 0.001194\n",
      "Step 400 \t Loss 0.000885\n",
      "Step 500 \t Loss 0.000627\n",
      "Step 600 \t Loss 0.000420\n",
      "Step 700 \t Loss 0.000255\n",
      "Step 800 \t Loss 0.000134\n",
      "Step 900 \t Loss 0.000053\n"
     ]
    }
   ],
   "source": [
    "steps = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "w = mlp.initialize(key)\n",
    "\n",
    "for step in range(steps):\n",
    "    # compute outputs and activations\n",
    "    outputs, activations = mlp(inputs, w)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = error(outputs, targets)\n",
    "    \n",
    "    # compute error gradient\n",
    "    error_grad = error.grad(outputs, targets)\n",
    "    \n",
    "    # compute gradient of weights\n",
    "    grad_w, _ = mlp.backward(w, activations, error_grad)\n",
    "    \n",
    "    # dualize gradient\n",
    "    d_w = mlp.dualize(grad_w)\n",
    "\n",
    "    # compute scheduled learning rate\n",
    "    lr = learning_rate * (1 - step / steps)\n",
    "    \n",
    "    # update weights\n",
    "    w = [weight - lr * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:3d} \\t Loss {loss:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
