{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, GPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to build a transformer. In particular, we'll see how to define attention and residual blocks in Modula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the Shakespeare dataset. The task will be to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shakespeare dataset...\n",
      "Processing Shakespeare dataset...\n",
      "Length of dataset in characters: 1,115,394\n",
      "Vocabulary size: 65\n",
      "Train has 1,003,854 tokens\n",
      "Val has 111,540 tokens\n",
      "Shakespeare dataset processing complete.\n"
     ]
    }
   ],
   "source": [
    "context = 64\n",
    "batch_size = 12\n",
    "\n",
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at an example to verify the data loaded correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (12, 64)\n",
      "Target shape: (12, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42] ...\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1] ...\n",
      "\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "\n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0][:10], \"...\")\n",
    "    print(\"First target sequence:\", targets[0][:10], \"...\")\n",
    "    print(\"\\nDecoded input:\", decode(inputs[0]))\n",
    "    print(\"\\nDecoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the architecture\n",
    "\n",
    "Let's use a very small setting for our transformer so it is fast to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer hyperparameters\n",
    "\n",
    "vocab_size = 65\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "d_query = 32\n",
    "d_value = 32\n",
    "num_blocks = 4\n",
    "attention_scale = 1\n",
    "final_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.1\n",
    "beta = 0.95\n",
    "steps = 2001\n",
    "log_interval = 10\n",
    "val_interval = 100\n",
    "val_iters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next up, we'll define the *attention* module and *residual blocks*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention in Modula\n",
    "\n",
    "In Modula, we'll define attention by stringing together several bond modules to do the parameterless computations. The roadmap is:\n",
    "* Map `(batch, token, d_embed)` into `(batch, head, token, d_query)` (and same for key and value) via `Linear` and `SplitIntoHeads`\n",
    "* Use Rotary Positional Embeddings (RoPE) on the query and the key via `Rope`\n",
    "* Map `query` and `key` into attention similarities of shape `(batch, head, token, token)` via `AttentionQK`\n",
    "* Use a causal mask and then softmax to create attention scores via `CausalMask` and `Softmax`\n",
    "* Use the attention scores to create output vectors via `ApplyAttentionScores`, then `MergeHeads` and `Linear`\n",
    "\n",
    "The main difference to a standard transformer is that `AttentionQK` uses $1/d_\\text{head}$ scaling instead of the standard $1/\\sqrt{d_\\text{head}}$. The reason for this is to provide Lipschitz guarantees for attention that are independent of $d_\\text{head}$. For more information on this, see Appendix B.6 of [Scalable Optimization in the Modular Norm](https://arxiv.org/pdf/2405.14813).\n",
    "\n",
    "And here's the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.atom import Linear\n",
    "from modula.bond import SplitIntoHeads, MergeHeads, Rope, AttentionQK, CausalMask, Softmax, ApplyAttentionScores, GeLU\n",
    "\n",
    "def Attention(num_heads, d_embed, d_query, d_value, attention_scale):\n",
    "    \"\"\"Multi-head attention\"\"\"\n",
    "\n",
    "    # For keys, queries, and values we add a heads dimension. For the out projection, we remove heads.\n",
    "    # Remember modules compose right-to-left, and the order is Linear(d_out, d_in)! And @ means compose.\n",
    "    Q = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    K = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    V = SplitIntoHeads(num_heads) @ Linear(num_heads * d_value, d_embed)\n",
    "    W = Linear(d_embed, num_heads * d_value) @ MergeHeads()\n",
    "\n",
    "    # Read right-to-left: rotate (Q, K) with RoPE, apply Q @ K.T, mask, softmax (with a scale we can choose).\n",
    "    AttentionScores = Softmax(attention_scale) @ CausalMask() @ AttentionQK() @ Rope(d_query) @ (Q, K)\n",
    "\n",
    "    # Read right-to-left: apply attention scores, multiply by 1/3 to fix the sensitivity to 1, project back to d_embed.\n",
    "    return W @ (1/3 * ApplyAttentionScores()) @ (V, AttentionScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the sensitivity is 1 at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 4 atoms and 10 bonds\n",
      "...smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 4 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "print(Attention(num_heads, d_embed, d_query, d_value, attention_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual blocks in Modula\n",
    "\n",
    "To implement the rest of our transformer, the roadmap is:\n",
    "* Embed the input tokens\n",
    "* Apply residual blocks for attention and the MLP\n",
    "* Project out\n",
    "\n",
    "All that's left is to set up the residual blocks. In Modula, we define residual connections using a convex combination. If $L$ is the number of residual blocks, then we use a convex combination of the identity and the block to get $x \\mapsto \\frac{L-1}{L} \\cdot x + \\frac{1}{L} \\cdot \\textsf{block}(x)$. The purpose is to create a Lipschitz guarantee that is independent of the number of blocks. For more information, see Proposition 4 of [Scalable Optimization in the Modular Norm](https://arxiv.org/pdf/2405.14813).\n",
    "\n",
    "In short, these changes enable Lipschitz guarantees on our transformer even as we scale the width and the depth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.abstract import Identity\n",
    "from modula.atom import Embed\n",
    "\n",
    "def GPT(vocab_size, num_heads, d_embed, d_query, d_value, num_blocks, blocks_mass=5, attention_scale=1.0, final_scale=1.0):\n",
    "    # Set embed to have mass 1. This controls the proportion of feature learning that it contributes to the whole network.\n",
    "    embed = Embed(d_embed, vocab_size)\n",
    "    embed.tare()\n",
    "\n",
    "    # Let's create attention and MLP layers. \n",
    "    att = Attention(num_heads, d_embed, d_query, d_value, attention_scale)\n",
    "    mlp = Linear(d_embed, 4*d_embed) @ GeLU() @ Linear(4*d_embed, d_embed)\n",
    "\n",
    "    # For our residual connections, L = 2*num_blocks because each block has two residual connections.\n",
    "    att_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * att\n",
    "    mlp_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * mlp\n",
    "\n",
    "    # We can use powers of a module to compose it with itself many times!\n",
    "    blocks = (mlp_block @ att_block) ** num_blocks\n",
    "\n",
    "    # Set all transformer blocks to have mass 5 (by default).\n",
    "    # So 5/7 of the change in the network output is due to the blocks,\n",
    "    # and 2/7 of the change in output is due to the embedding and out projection.\n",
    "    blocks.tare(absolute=blocks_mass)\n",
    "\n",
    "    out = final_scale * Linear(vocab_size, d_embed)\n",
    "\n",
    "    return out @ blocks @ embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we are ready to construct our GPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 26 atoms and 78 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 7.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    d_query=d_query,\n",
    "    d_value=d_value,\n",
    "    num_blocks=num_blocks,\n",
    "    attention_scale=attention_scale,\n",
    "    final_scale=final_scale,\n",
    ")\n",
    "\n",
    "model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and training\n",
    "\n",
    "To train our transformer we'll use cross entropy loss, which we can compute by decomposing the softmax:\n",
    "\n",
    "$$\n",
    "-\\log(\\text{target probability}) = -\\log(\\text{softmax}(\\text{logits})_\\text{target}) = -\\text{logit}_\\text{target} + \\text{log\\,sum\\,exp}(\\text{logits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    # We use the logsumexp trick for stable cross entropy\n",
    "    logits = model(inputs, w)  # shape is [batch, seq_len, vocab_size]\n",
    "    batch_indices = jnp.arange(logits.shape[0])[:, None]  # shape is [batch, 1]\n",
    "    seq_indices = jnp.arange(logits.shape[1])[None, :]    # shape is [1, seq_len]\n",
    "    # This indexing selects out logits[b, s, targets[b, s]], which is the target logit\n",
    "    losses = -logits[batch_indices, seq_indices, targets] + jax.nn.logsumexp(logits, axis=-1)  # shape is [batch, seq_len]\n",
    "    return losses.mean()\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.226325988769531\n",
      "--> val loss 4.179544448852539\n",
      "Step 10: loss 3.8738746643066406\n",
      "Step 20: loss 3.3448646068573\n",
      "Step 30: loss 2.805002212524414\n",
      "Step 40: loss 2.68573260307312\n",
      "Step 50: loss 2.6098480224609375\n",
      "Step 60: loss 2.407468557357788\n",
      "Step 70: loss 2.418379783630371\n",
      "Step 80: loss 2.359757423400879\n",
      "Step 90: loss 2.2685279846191406\n",
      "Step 100: loss 2.314124584197998\n",
      "--> val loss 2.541980743408203\n",
      "Step 110: loss 2.283424139022827\n",
      "Step 120: loss 2.2063167095184326\n",
      "Step 130: loss 2.1598031520843506\n",
      "Step 140: loss 2.252727508544922\n",
      "Step 150: loss 2.124152660369873\n",
      "Step 160: loss 2.23785662651062\n",
      "Step 170: loss 2.2059123516082764\n",
      "Step 180: loss 2.102996587753296\n",
      "Step 190: loss 2.132392168045044\n",
      "Step 200: loss 2.130244255065918\n",
      "--> val loss 2.359212636947632\n",
      "Step 210: loss 2.0895276069641113\n",
      "Step 220: loss 2.1278815269470215\n",
      "Step 230: loss 1.9647449254989624\n",
      "Step 240: loss 2.1118733882904053\n",
      "Step 250: loss 1.9459623098373413\n",
      "Step 260: loss 2.118051290512085\n",
      "Step 270: loss 2.0605385303497314\n",
      "Step 280: loss 2.0378551483154297\n",
      "Step 290: loss 2.0237479209899902\n",
      "Step 300: loss 1.982785940170288\n",
      "--> val loss 2.2887392044067383\n",
      "Step 310: loss 2.073058605194092\n",
      "Step 320: loss 2.082066535949707\n",
      "Step 330: loss 2.130162239074707\n",
      "Step 340: loss 2.092909336090088\n",
      "Step 350: loss 1.9229984283447266\n",
      "Step 360: loss 1.9037134647369385\n",
      "Step 370: loss 2.0083131790161133\n",
      "Step 380: loss 2.0236263275146484\n",
      "Step 390: loss 2.0116419792175293\n",
      "Step 400: loss 2.091407299041748\n",
      "--> val loss 2.2199790477752686\n",
      "Step 410: loss 2.0855846405029297\n",
      "Step 420: loss 1.8506882190704346\n",
      "Step 430: loss 1.9745848178863525\n",
      "Step 440: loss 1.9135173559188843\n",
      "Step 450: loss 2.0486648082733154\n",
      "Step 460: loss 1.983982801437378\n",
      "Step 470: loss 1.9958977699279785\n",
      "Step 480: loss 1.9868993759155273\n",
      "Step 490: loss 2.009216785430908\n",
      "Step 500: loss 2.073169231414795\n",
      "--> val loss 2.141632556915283\n",
      "Step 510: loss 2.0603322982788086\n",
      "Step 520: loss 2.0025858879089355\n",
      "Step 530: loss 1.9482192993164062\n",
      "Step 540: loss 1.9092429876327515\n",
      "Step 550: loss 2.109374761581421\n",
      "Step 560: loss 1.9060167074203491\n",
      "Step 570: loss 1.9423940181732178\n",
      "Step 580: loss 1.9405231475830078\n",
      "Step 590: loss 1.9132475852966309\n",
      "Step 600: loss 2.0125274658203125\n",
      "--> val loss 2.2273831367492676\n",
      "Step 610: loss 2.0854687690734863\n",
      "Step 620: loss 1.9796791076660156\n",
      "Step 630: loss 1.982351303100586\n",
      "Step 640: loss 2.044363021850586\n",
      "Step 650: loss 2.030698299407959\n",
      "Step 660: loss 2.0731544494628906\n",
      "Step 670: loss 1.9660027027130127\n",
      "Step 680: loss 1.933128833770752\n",
      "Step 690: loss 1.8852118253707886\n",
      "Step 700: loss 1.8401598930358887\n",
      "--> val loss 2.0958476066589355\n",
      "Step 710: loss 1.9790323972702026\n",
      "Step 720: loss 2.0329394340515137\n",
      "Step 730: loss 1.929424524307251\n",
      "Step 740: loss 1.950282335281372\n",
      "Step 750: loss 1.938680648803711\n",
      "Step 760: loss 1.9717748165130615\n",
      "Step 770: loss 1.8411779403686523\n",
      "Step 780: loss 2.085500717163086\n",
      "Step 790: loss 1.8778104782104492\n",
      "Step 800: loss 1.9712986946105957\n",
      "--> val loss 2.1469686031341553\n",
      "Step 810: loss 1.949462652206421\n",
      "Step 820: loss 1.9898126125335693\n",
      "Step 830: loss 1.9045312404632568\n",
      "Step 840: loss 1.9053363800048828\n",
      "Step 850: loss 1.8944416046142578\n",
      "Step 860: loss 1.8389015197753906\n",
      "Step 870: loss 1.9189136028289795\n",
      "Step 880: loss 2.0141639709472656\n",
      "Step 890: loss 1.9987534284591675\n",
      "Step 900: loss 1.947631597518921\n",
      "--> val loss 2.1903281211853027\n",
      "Step 910: loss 2.031083106994629\n",
      "Step 920: loss 1.988853931427002\n",
      "Step 930: loss 2.0356318950653076\n",
      "Step 940: loss 1.8823192119598389\n",
      "Step 950: loss 2.0429515838623047\n",
      "Step 960: loss 2.021817684173584\n",
      "Step 970: loss 2.003168821334839\n",
      "Step 980: loss 2.0105528831481934\n",
      "Step 990: loss 2.014195680618286\n",
      "Step 1000: loss 1.9518741369247437\n",
      "--> val loss 2.0813283920288086\n",
      "Step 1010: loss 2.016996383666992\n",
      "Step 1020: loss 2.04374098777771\n",
      "Step 1030: loss 1.8839387893676758\n",
      "Step 1040: loss 1.96620512008667\n",
      "Step 1050: loss 2.0463950634002686\n",
      "Step 1060: loss 1.9169645309448242\n",
      "Step 1070: loss 2.038651943206787\n",
      "Step 1080: loss 2.0474071502685547\n",
      "Step 1090: loss 1.9452462196350098\n",
      "Step 1100: loss 1.8884999752044678\n",
      "--> val loss 2.1541106700897217\n",
      "Step 1110: loss 1.9775495529174805\n",
      "Step 1120: loss 1.96068274974823\n",
      "Step 1130: loss 1.8553755283355713\n",
      "Step 1140: loss 1.9422013759613037\n",
      "Step 1150: loss 2.0833449363708496\n",
      "Step 1160: loss 1.840619444847107\n",
      "Step 1170: loss 2.032219409942627\n",
      "Step 1180: loss 1.9345749616622925\n",
      "Step 1190: loss 1.934565544128418\n",
      "Step 1200: loss 1.9528722763061523\n",
      "--> val loss 2.1688506603240967\n",
      "Step 1210: loss 1.8676490783691406\n",
      "Step 1220: loss 1.9311145544052124\n",
      "Step 1230: loss 1.9905321598052979\n",
      "Step 1240: loss 1.8773740530014038\n",
      "Step 1250: loss 1.9832658767700195\n",
      "Step 1260: loss 1.8256521224975586\n",
      "Step 1270: loss 2.037313461303711\n",
      "Step 1280: loss 1.9440114498138428\n",
      "Step 1290: loss 1.9472723007202148\n",
      "Step 1300: loss 1.862718105316162\n",
      "--> val loss 2.0632894039154053\n",
      "Step 1310: loss 1.944453239440918\n",
      "Step 1320: loss 1.869157075881958\n",
      "Step 1330: loss 1.9843480587005615\n",
      "Step 1340: loss 1.9083728790283203\n",
      "Step 1350: loss 1.920233130455017\n",
      "Step 1360: loss 1.7926225662231445\n",
      "Step 1370: loss 1.8765363693237305\n",
      "Step 1380: loss 1.9374698400497437\n",
      "Step 1390: loss 1.9032771587371826\n",
      "Step 1400: loss 1.8976068496704102\n",
      "--> val loss 2.0361690521240234\n",
      "Step 1410: loss 1.8799960613250732\n",
      "Step 1420: loss 1.9112414121627808\n",
      "Step 1430: loss 1.8797309398651123\n",
      "Step 1440: loss 1.9040837287902832\n",
      "Step 1450: loss 1.8828296661376953\n",
      "Step 1460: loss 1.83419930934906\n",
      "Step 1470: loss 1.8327134847640991\n",
      "Step 1480: loss 1.857541799545288\n",
      "Step 1490: loss 1.8209788799285889\n",
      "Step 1500: loss 1.780470371246338\n",
      "--> val loss 2.0466208457946777\n",
      "Step 1510: loss 1.8544996976852417\n",
      "Step 1520: loss 1.8710064888000488\n",
      "Step 1530: loss 1.8195044994354248\n",
      "Step 1540: loss 1.874974250793457\n",
      "Step 1550: loss 1.7101812362670898\n",
      "Step 1560: loss 1.8439801931381226\n",
      "Step 1570: loss 1.967679500579834\n",
      "Step 1580: loss 1.888682246208191\n",
      "Step 1590: loss 1.6926288604736328\n",
      "Step 1600: loss 1.875901222229004\n",
      "--> val loss 2.044935941696167\n",
      "Step 1610: loss 1.8210939168930054\n",
      "Step 1620: loss 1.7439773082733154\n",
      "Step 1630: loss 1.7956527471542358\n",
      "Step 1640: loss 1.792572021484375\n",
      "Step 1650: loss 1.7985519170761108\n",
      "Step 1660: loss 1.8520288467407227\n",
      "Step 1670: loss 1.680544137954712\n",
      "Step 1680: loss 1.7917392253875732\n",
      "Step 1690: loss 1.8400462865829468\n",
      "Step 1700: loss 1.6793416738510132\n",
      "--> val loss 1.995697021484375\n",
      "Step 1710: loss 1.7414367198944092\n",
      "Step 1720: loss 1.8606326580047607\n",
      "Step 1730: loss 1.7578084468841553\n",
      "Step 1740: loss 1.6292760372161865\n",
      "Step 1750: loss 1.7017428874969482\n",
      "Step 1760: loss 1.8407533168792725\n",
      "Step 1770: loss 1.7789411544799805\n",
      "Step 1780: loss 1.802499532699585\n",
      "Step 1790: loss 1.7586851119995117\n",
      "Step 1800: loss 1.7281568050384521\n",
      "--> val loss 1.9875770807266235\n",
      "Step 1810: loss 1.7767337560653687\n",
      "Step 1820: loss 1.7158925533294678\n",
      "Step 1830: loss 1.7596324682235718\n",
      "Step 1840: loss 1.7826766967773438\n",
      "Step 1850: loss 1.7769875526428223\n",
      "Step 1860: loss 1.6953961849212646\n",
      "Step 1870: loss 1.7714271545410156\n",
      "Step 1880: loss 1.6994340419769287\n",
      "Step 1890: loss 1.7252253293991089\n",
      "Step 1900: loss 1.566367506980896\n",
      "--> val loss 1.9310436248779297\n",
      "Step 1910: loss 1.7057380676269531\n",
      "Step 1920: loss 1.7441104650497437\n",
      "Step 1930: loss 1.7951183319091797\n",
      "Step 1940: loss 1.8611491918563843\n",
      "Step 1950: loss 1.787139654159546\n",
      "Step 1960: loss 1.788725733757019\n",
      "Step 1970: loss 1.7919573783874512\n",
      "Step 1980: loss 1.706597089767456\n",
      "Step 1990: loss 1.771501898765564\n",
      "Step 2000: loss 1.7121562957763672\n",
      "--> val loss 1.8968441486358643\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "lr_schedule = lambda step: lr * (steps - step) / steps\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    w = [weight - lr_schedule(step) * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "    \n",
    "    if step % val_interval == 0:\n",
    "        val_losses = []\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "            val_losses.append(loss)\n",
    "            if len(val_losses) >= val_iters:\n",
    "                break\n",
    "        print(f\"--> val loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Though this be madness, yet there is method in't\n",
    "\n",
    "And indeed, let us look at how our wee model stacks up to the master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "\n",
      "If where his elperiend and is here in think the comfore be pray virtue deather I the grouth a pears my\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 1:\n",
      "\n",
      "If as the conture the weet to the man's death the greeen he with thought rame the prosates he palousen\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2:\n",
      "\n",
      "If him the be not me were and let for the earth the forth,\n",
      "That the his a wort of you the fearshould a\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=100, temperature=0.5, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    tokens = jnp.array(encode(prompt))\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(jnp.expand_dims(tokens, 0), w)\n",
    "        next_token_logits = logits[0, -1] / temperature\n",
    "        \n",
    "        # Sample from our model's token distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        next_token = jax.random.categorical(subkey, next_token_logits)\n",
    "        tokens = jnp.append(tokens, next_token)\n",
    "    \n",
    "    return decode(tokens)\n",
    "\n",
    "for seed in range(3):\n",
    "    print(f\"Sample {seed}:\\n\\n{generate_text('If', max_tokens=100, seed=seed)}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
