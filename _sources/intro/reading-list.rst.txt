Reading list
=============

Core papers
------------

- `Scalable optimization in the modular norm <https://arxiv.org/abs/2405.14813>`_
- `Modular duality in deep learning <https://arxiv.org/abs/2410.21265>`_

Optimization
-------------

- `Preconditioned spectral descent for deep learning <https://papers.nips.cc/paper_files/paper/2015/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html>`_
- `The duality structure gradient descent algorithm: analysis and applications to neural networks <https://arxiv.org/abs/1708.00523>`_
- `On the distance between two neural networks and the stability of learning <https://arxiv.org/abs/2002.03432>`_
- `Automatic gradient descent: Deep learning without hyperparameters <https://arxiv.org/abs/2304.05187>`_
- `A spectral condition for feature learning <https://arxiv.org/abs/2310.17813>`_ 
- `Universal majorization-minimization algorithms <https://arxiv.org/abs/2308.00190>`_
- `Old optimizer, new norm: An anthology <https://arxiv.org/abs/2409.20325>`_
- `Muon: An optimizer for hidden layers in neural networks <https://kellerjordan.github.io/posts/muon/>`_

Generalization
---------------

- `Spectrally-normalized margin bounds for neural networks <https://arxiv.org/abs/1706.08498>`_
- `A PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks <https://arxiv.org/abs/1707.09564>`_
- `Investigating generalization by controlling normalized margin <https://arxiv.org/abs/2205.03940>`_

New developments
-----------------

- `Preconditioning and normalization in optimizing deep neural networks <https://github.com/ZQZCalin/trainit/blob/master/optimizers/muon/mango_report.pdf>`_
- `Improving SOAP using iterative whitening and Muon <https://nikhilvyas.github.io/SOAP_Muon.pdf>`_
- `On the concurrence of layer-wise preconditioning methods and provable feature learning <https://arxiv.org/abs/2502.01763>`_
- `A note on the convergence of Muon and further <https://arxiv.org/abs/2502.02900>`_
- `Training deep learning models with norm-constrained LMOs <https://arxiv.org/abs/2502.07529>`_
- `Muon is scalable for LLM training <https://arxiv.org/abs/2502.16982>`_
